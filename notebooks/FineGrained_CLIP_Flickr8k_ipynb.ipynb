{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Mount Google Drive (Colab Only)\n",
        "# ============================================================\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Change this path only if your dataset location changes\n",
        "DRIVE_ROOT = '/content/drive/MyDrive/SIH/ML/flickr8k'\n",
        "print(\"Using DRIVE_ROOT =\", DRIVE_ROOT)"
      ],
      "metadata": {
        "id": "pZDjg1mb2uGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Install OpenAI CLIP (Required for Colab)\n",
        "# ============================================================\n",
        "\n",
        "!pip install --upgrade pip\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "id": "ZutlL4bP3wav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Imports, Device Setup, and Configuration\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import time\n",
        "import csv\n",
        "import re\n",
        "import math\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import clip\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ============================================================\n",
        "# Configuration (Matches Your Original Notebook Exactly)\n",
        "# ============================================================\n",
        "\n",
        "class CFG:\n",
        "    model_name = 'ViT-B/32'\n",
        "    image_size = 224\n",
        "    batch_size = 64\n",
        "    epochs = 12\n",
        "    lr = 1e-5\n",
        "    weight_decay = 1e-4\n",
        "    temperature = 0.07\n",
        "    freeze_backbone_layers = False\n",
        "    num_workers = 4\n",
        "    pin_memory = True\n",
        "    seed = 42\n",
        "    device = device\n",
        "\n",
        "cfg = CFG()\n",
        "torch.manual_seed(cfg.seed)"
      ],
      "metadata": {
        "id": "RN0BKM-t3yF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Flickr8k Dataset (Robust CSV-Aware Parser)\n",
        "# ============================================================\n",
        "\n",
        "class Flickr8kDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Robust parser for Flickr8k captions.txt.\n",
        "    Handles:\n",
        "    - CSV format with header\n",
        "    - .jpg#0 format\n",
        "    - Tab separated\n",
        "    - Space separated fallback\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, root: str, split='train', transform=None,\n",
        "                 max_captions_per_image=5, verbose=True):\n",
        "\n",
        "        self.root = Path(root)\n",
        "        captions_path = self.root / 'captions.txt'\n",
        "        images_dir = self.root / 'Images'\n",
        "\n",
        "        if not captions_path.exists():\n",
        "            raise FileNotFoundError(f\"Captions file not found at {captions_path}\")\n",
        "\n",
        "        if not images_dir.exists():\n",
        "            raise FileNotFoundError(f\"Images folder not found at {images_dir}\")\n",
        "\n",
        "        self.image2caps = {}\n",
        "        self.transform = transform\n",
        "\n",
        "        # ---------- CSV parsing ----------\n",
        "        with open(captions_path, 'r', encoding='utf-8') as f:\n",
        "            reader = csv.reader(f)\n",
        "            rows = list(reader)\n",
        "\n",
        "        start_idx = 0\n",
        "        if rows and rows[0][0].strip().lower() in ('image','filename'):\n",
        "            start_idx = 1\n",
        "\n",
        "        for row in rows[start_idx:]:\n",
        "            if len(row) < 2:\n",
        "                continue\n",
        "            imgname = row[0].split('#')[0].strip()\n",
        "            caption = ','.join(row[1:]).strip()\n",
        "            self.image2caps.setdefault(imgname, []).append(caption)\n",
        "\n",
        "        # ---------- Load images ----------\n",
        "        self.images = sorted(list(images_dir.glob('*.jpg')))\n",
        "\n",
        "        # ---------- Deterministic split ----------\n",
        "        n = len(self.images)\n",
        "        train_end = int(0.8 * n)\n",
        "        val_end = int(0.9 * n)\n",
        "\n",
        "        if split == 'train':\n",
        "            images_subset = self.images[:train_end]\n",
        "        elif split == 'val':\n",
        "            images_subset = self.images[train_end:val_end]\n",
        "        elif split == 'test':\n",
        "            images_subset = self.images[val_end:]\n",
        "        else:\n",
        "            images_subset = self.images\n",
        "\n",
        "        # ---------- Expand into (image, caption) pairs ----------\n",
        "        self.pairs = []\n",
        "        for p in images_subset:\n",
        "            caps = self.image2caps.get(p.name, [])\n",
        "            for c in caps[:max_captions_per_image]:\n",
        "                self.pairs.append((str(p), c))\n",
        "\n",
        "        print(f\"{split} pairs:\", len(self.pairs))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, caption = self.pairs[idx]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, caption, os.path.basename(img_path)"
      ],
      "metadata": {
        "id": "euVuXmLo30Fl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Image Preprocessing and DataLoaders\n",
        "# ============================================================\n",
        "\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((cfg.image_size, cfg.image_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=(0.48145466, 0.4578275, 0.40821073),\n",
        "        std=(0.26862954, 0.26130258, 0.27577711)\n",
        "    )\n",
        "])\n",
        "\n",
        "train_ds = Flickr8kDataset(DRIVE_ROOT, split='train', transform=preprocess)\n",
        "val_ds   = Flickr8kDataset(DRIVE_ROOT, split='val', transform=preprocess)\n",
        "\n",
        "train_loader = DataLoader(train_ds,\n",
        "                          batch_size=cfg.batch_size,\n",
        "                          shuffle=True,\n",
        "                          num_workers=cfg.num_workers,\n",
        "                          pin_memory=cfg.pin_memory)\n",
        "\n",
        "val_loader = DataLoader(val_ds,\n",
        "                        batch_size=cfg.batch_size,\n",
        "                        shuffle=False,\n",
        "                        num_workers=cfg.num_workers,\n",
        "                        pin_memory=cfg.pin_memory)"
      ],
      "metadata": {
        "id": "icA07ctI31qp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CLIP Wrapper with Projection Heads\n",
        "# ============================================================\n",
        "\n",
        "class FineTuneCLIP(nn.Module):\n",
        "\n",
        "    def __init__(self, clip_model_name='ViT-B/32',\n",
        "                 embed_dim=512, proj_dim=256):\n",
        "        super().__init__()\n",
        "\n",
        "        self.clip_model, _ = clip.load(clip_model_name, device='cpu')\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        # Projection head for image embeddings\n",
        "        self.img_proj = nn.Sequential(\n",
        "            nn.Linear(self.clip_model.visual.output_dim, proj_dim),\n",
        "            nn.LayerNorm(proj_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(proj_dim, embed_dim)\n",
        "        )\n",
        "\n",
        "        # Projection head for text embeddings\n",
        "        self.txt_proj = nn.Sequential(\n",
        "            nn.Linear(self.clip_model.transformer.width, proj_dim),\n",
        "            nn.LayerNorm(proj_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(proj_dim, embed_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, images, tokenized_text):\n",
        "\n",
        "        img_features = self.clip_model.encode_image(images)\n",
        "        txt_features = self.clip_model.encode_text(tokenized_text)\n",
        "\n",
        "        img_emb = F.normalize(self.img_proj(img_features), dim=-1)\n",
        "        txt_emb = F.normalize(self.txt_proj(txt_features), dim=-1)\n",
        "\n",
        "        return img_emb, txt_emb\n",
        "\n",
        "\n",
        "model = FineTuneCLIP(cfg.model_name).to(device)\n",
        "model.clip_model.to(device)"
      ],
      "metadata": {
        "id": "yq9fRvSF32_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Contrastive Loss (NT-Xent)\n",
        "# ============================================================\n",
        "\n",
        "class NTXentLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, temperature=0.07):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, img_emb, txt_emb):\n",
        "\n",
        "        logits = img_emb @ txt_emb.t() / self.temperature\n",
        "        labels = torch.arange(logits.size(0), device=logits.device)\n",
        "\n",
        "        loss_i2t = F.cross_entropy(logits, labels)\n",
        "        loss_t2i = F.cross_entropy(logits.t(), labels)\n",
        "\n",
        "        return (loss_i2t + loss_t2i) / 2\n",
        "\n",
        "\n",
        "criterion = NTXentLoss(cfg.temperature)\n",
        "optimizer = torch.optim.AdamW(model.parameters(),\n",
        "                              lr=cfg.lr,\n",
        "                              weight_decay=cfg.weight_decay)"
      ],
      "metadata": {
        "id": "4ObSDyt334a7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Stage 1: Global Contrastive Fine-Tuning\n",
        "# (Exactly same NT-Xent training as your original notebook)\n",
        "# ============================================================\n",
        "\n",
        "epochs = cfg.epochs\n",
        "best_val_r1 = -1.0\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\")\n",
        "\n",
        "    for images, captions, _ in pbar:\n",
        "\n",
        "        images = images.to(device)\n",
        "        tokenized = clip.tokenize(list(captions), truncate=True).to(device)\n",
        "\n",
        "        img_emb, txt_emb = model(images, tokenized)\n",
        "        loss = criterion(img_emb, txt_emb)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        pbar.set_postfix({'loss': f\"{epoch_loss/(pbar.n+1):.4f}\"})\n",
        "\n",
        "    print(f\"Epoch {epoch} Avg Loss:\", epoch_loss/len(train_loader))\n",
        "\n",
        "    # Save checkpoint\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'cfg': vars(cfg)\n",
        "    }, f\"clip_finetuned_epoch{epoch}.pth\")\n",
        "\n",
        "print(\"Stage 1 training complete.\")"
      ],
      "metadata": {
        "id": "FjRnvzCg4KDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Retrieval Evaluation (Exact Same Logic as Original)\n",
        "# ============================================================\n",
        "\n",
        "def compute_retrieval(model, dataset):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    image_paths = []\n",
        "    captions = []\n",
        "    caption_image_idx = []\n",
        "    img_name_to_idx = {}\n",
        "\n",
        "    # Build mapping\n",
        "    for img_path, cap in dataset.pairs:\n",
        "        name = os.path.basename(img_path)\n",
        "        if name not in img_name_to_idx:\n",
        "            img_name_to_idx[name] = len(image_paths)\n",
        "            image_paths.append(img_path)\n",
        "        captions.append(cap)\n",
        "        caption_image_idx.append(img_name_to_idx[name])\n",
        "\n",
        "    # Encode images\n",
        "    image_embs = []\n",
        "    with torch.no_grad():\n",
        "        for p in image_paths:\n",
        "            im = preprocess(Image.open(p).convert('RGB')).unsqueeze(0).to(device)\n",
        "            img_feat = model.clip_model.encode_image(im)\n",
        "            img_feat = model.img_proj(img_feat)\n",
        "            image_embs.append(F.normalize(img_feat, dim=-1).cpu())\n",
        "\n",
        "    image_embs = torch.cat(image_embs)\n",
        "\n",
        "    # Encode captions\n",
        "    text_embs = []\n",
        "    with torch.no_grad():\n",
        "        for cap in captions:\n",
        "            tokenized = clip.tokenize([cap]).to(device)\n",
        "            txt_feat = model.clip_model.encode_text(tokenized)\n",
        "            txt_feat = model.txt_proj(txt_feat)\n",
        "            text_embs.append(F.normalize(txt_feat, dim=-1).cpu())\n",
        "\n",
        "    text_embs = torch.cat(text_embs)\n",
        "\n",
        "    sim = image_embs @ text_embs.t()\n",
        "    sims = sim.numpy()\n",
        "\n",
        "    # Image -> Text recall\n",
        "    def recall_i2t(k):\n",
        "        correct = 0\n",
        "        for i in range(len(image_paths)):\n",
        "            ranked = np.argsort(-sims[i])[:k]\n",
        "            if any(caption_image_idx[j] == i for j in ranked):\n",
        "                correct += 1\n",
        "        return correct / len(image_paths) * 100\n",
        "\n",
        "    # Text -> Image recall\n",
        "    def recall_t2i(k):\n",
        "        correct = 0\n",
        "        for j in range(len(captions)):\n",
        "            ranked = np.argsort(-sims[:, j])[:k]\n",
        "            if caption_image_idx[j] in ranked:\n",
        "                correct += 1\n",
        "        return correct / len(captions) * 100\n",
        "\n",
        "    print(\"Image->Text R@1/5/10:\",\n",
        "          recall_i2t(1), recall_i2t(5), recall_i2t(10))\n",
        "    print(\"Text->Image R@1/5/10:\",\n",
        "          recall_t2i(1), recall_t2i(5), recall_t2i(10))\n",
        "\n",
        "\n",
        "compute_retrieval(model, val_ds)"
      ],
      "metadata": {
        "id": "QrrgnkBK4Lqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Add Fine-Grained Projection Layers (Patch + Token)\n",
        "# ============================================================\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# Infer token dimensions\n",
        "with torch.no_grad():\n",
        "    for imgs, caps, _ in train_loader:\n",
        "        imgs = imgs[:2].to(device)\n",
        "        tokenized = clip.tokenize(list(caps[:2]), truncate=True).to(device)\n",
        "\n",
        "        # Patch tokens\n",
        "        x = model.clip_model.visual.conv1(imgs)\n",
        "        x = x.reshape(x.shape[0], x.shape[1], -1).permute(0,2,1)\n",
        "        cls = model.clip_model.visual.class_embedding.to(x.dtype)\\\n",
        "              .unsqueeze(0).unsqueeze(0).expand(x.shape[0], -1, -1)\n",
        "        x = torch.cat([cls, x], dim=1)\n",
        "        x = x + model.clip_model.visual.positional_embedding.to(x.dtype)\n",
        "        x = model.clip_model.visual.ln_pre(x)\n",
        "        x = x.permute(1,0,2)\n",
        "        x = model.clip_model.visual.transformer(x)\n",
        "        x = x.permute(1,0,2)\n",
        "        x = model.clip_model.visual.ln_post(x)\n",
        "        patch_tokens = x[:,1:,:]\n",
        "        D_patch = patch_tokens.shape[-1]\n",
        "\n",
        "        # Text tokens\n",
        "        t = model.clip_model.token_embedding(tokenized).type(torch.float32)\n",
        "        t = t + model.clip_model.positional_embedding.to(t.dtype)\n",
        "        t = t.permute(1,0,2)\n",
        "        t = model.clip_model.transformer(t)\n",
        "        t = t.permute(1,0,2)\n",
        "        token_tokens = model.clip_model.ln_final(t)\n",
        "        D_text = token_tokens.shape[-1]\n",
        "        break\n",
        "\n",
        "proj_dim = model.embed_dim\n",
        "\n",
        "model.patch_proj = nn.Linear(D_patch, proj_dim).to(device)\n",
        "model.token_proj = nn.Linear(D_text, proj_dim).to(device)\n",
        "\n",
        "print(\"Fine-grained projection heads added.\")"
      ],
      "metadata": {
        "id": "xzDpolsD4NWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Fine-Grained Similarity Matrix (Exact Original Logic)\n",
        "# ============================================================\n",
        "\n",
        "def extract_patch_tokens(clip_model, images):\n",
        "\n",
        "    x = clip_model.visual.conv1(images)\n",
        "    B, C, Hf, Wf = x.shape\n",
        "    x = x.reshape(B, C, -1).permute(0,2,1)\n",
        "    cls = clip_model.visual.class_embedding.to(x.dtype)\\\n",
        "          .unsqueeze(0).unsqueeze(0).expand(B, -1, -1)\n",
        "    x = torch.cat([cls, x], dim=1)\n",
        "    x = x + clip_model.visual.positional_embedding.to(x.dtype)\n",
        "    x = clip_model.visual.ln_pre(x)\n",
        "    x = x.permute(1,0,2)\n",
        "    x = clip_model.visual.transformer(x)\n",
        "    x = x.permute(1,0,2)\n",
        "    x = clip_model.visual.ln_post(x)\n",
        "\n",
        "    return x[:,1:,:]\n",
        "\n",
        "\n",
        "def extract_text_tokens(clip_model, tokenized):\n",
        "\n",
        "    x = clip_model.token_embedding(tokenized).type(torch.float32)\n",
        "    x = x + clip_model.positional_embedding.to(x.dtype)\n",
        "    x = x.permute(1,0,2)\n",
        "    x = clip_model.transformer(x)\n",
        "    x = x.permute(1,0,2)\n",
        "    x = clip_model.ln_final(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def compute_fine_sim_matrix(patch_tokens, token_tokens):\n",
        "\n",
        "    Bp, P, Dp = patch_tokens.shape\n",
        "    Bt, T, Dt = token_tokens.shape\n",
        "\n",
        "    patch_p = model.patch_proj(patch_tokens.reshape(-1, Dp))\\\n",
        "                .reshape(Bp, P, -1)\n",
        "    token_p = model.token_proj(token_tokens.reshape(-1, Dt))\\\n",
        "                .reshape(Bt, T, -1)\n",
        "\n",
        "    patch_p = F.normalize(patch_p, dim=-1)\n",
        "    token_p = F.normalize(token_p, dim=-1)\n",
        "\n",
        "    sims = torch.zeros((Bp, Bt), device=patch_p.device)\n",
        "\n",
        "    for i in range(Bp):\n",
        "        sim_token_patch = torch.einsum('pd,btd->btp',\n",
        "                                       patch_p[i], token_p)\n",
        "        max_over_patches = sim_token_patch.max(dim=2).values\n",
        "        sims[i] = max_over_patches.mean(dim=1)\n",
        "\n",
        "    return sims"
      ],
      "metadata": {
        "id": "tlgzZHP94O_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Stage 2: Fine-Grained Alignment Training\n",
        "# (Exact same λ * fine_loss combination)\n",
        "# ============================================================\n",
        "\n",
        "lambda_fine = 1.0\n",
        "fine_epochs = 6\n",
        "\n",
        "for epoch in range(1, fine_epochs + 1):\n",
        "\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    pbar = tqdm(train_loader, desc=f\"FineEpoch {epoch}\")\n",
        "\n",
        "    for images, captions, _ in pbar:\n",
        "\n",
        "        images = images.to(device)\n",
        "        tokenized = clip.tokenize(list(captions), truncate=True).to(device)\n",
        "\n",
        "        # Global loss\n",
        "        img_g = model.clip_model.encode_image(images)\n",
        "        txt_g = model.clip_model.encode_text(tokenized)\n",
        "\n",
        "        img_proj = F.normalize(model.img_proj(img_g), dim=-1)\n",
        "        txt_proj = F.normalize(model.txt_proj(txt_g), dim=-1)\n",
        "\n",
        "        global_loss = criterion(img_proj, txt_proj)\n",
        "\n",
        "        # Fine-grain loss\n",
        "        patch_tokens = extract_patch_tokens(model.clip_model, images)\n",
        "        token_tokens = extract_text_tokens(model.clip_model, tokenized)\n",
        "\n",
        "        fine_sim = compute_fine_sim_matrix(patch_tokens, token_tokens)\n",
        "        fine_loss = criterion(fine_sim, torch.eye(fine_sim.size(0)).to(device))\n",
        "\n",
        "        loss = global_loss + lambda_fine * fine_loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        pbar.set_postfix({'loss': f\"{epoch_loss/(pbar.n+1):.4f}\"})\n",
        "\n",
        "    print(\"Fine Epoch\", epoch, \"Loss:\", epoch_loss/len(train_loader))\n",
        "\n",
        "print(\"Fine-grain training complete.\")"
      ],
      "metadata": {
        "id": "lIpZfqTl4Qxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Final Retrieval Evaluation After Fine-Grain Training\n",
        "# ============================================================\n",
        "\n",
        "compute_retrieval(model, val_ds)"
      ],
      "metadata": {
        "id": "QINMq5Sj4Sbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Build and Cache Dataset Embeddings (for Fast Retrieval)\n",
        "# ============================================================\n",
        "\n",
        "dataset_for_index = val_ds   # change to train_ds if needed\n",
        "\n",
        "image_paths = []\n",
        "captions = []\n",
        "caption_image_idx = []\n",
        "img_name_to_idx = {}\n",
        "\n",
        "for img_path, cap in dataset_for_index.pairs:\n",
        "    name = os.path.basename(img_path)\n",
        "    if name not in img_name_to_idx:\n",
        "        img_name_to_idx[name] = len(image_paths)\n",
        "        image_paths.append(img_path)\n",
        "    captions.append(cap)\n",
        "    caption_image_idx.append(img_name_to_idx[name])\n",
        "\n",
        "num_images = len(image_paths)\n",
        "num_captions = len(captions)\n",
        "\n",
        "print(\"Index built:\")\n",
        "print(\"Images:\", num_images)\n",
        "print(\"Captions:\", num_captions)\n",
        "\n",
        "# Encode images\n",
        "image_embs = []\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for p in tqdm(image_paths, desc=\"Encoding Images\"):\n",
        "        im = preprocess(Image.open(p).convert('RGB')).unsqueeze(0).to(device)\n",
        "        img_feat = model.clip_model.encode_image(im)\n",
        "        img_feat = model.img_proj(img_feat)\n",
        "        img_feat = F.normalize(img_feat, dim=-1).cpu()\n",
        "        image_embs.append(img_feat)\n",
        "\n",
        "image_embs = torch.cat(image_embs, dim=0)\n",
        "\n",
        "# Encode captions\n",
        "text_embs = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for cap in tqdm(captions, desc=\"Encoding Captions\"):\n",
        "        tokenized = clip.tokenize([cap], truncate=True).to(device)\n",
        "        txt_feat = model.clip_model.encode_text(tokenized)\n",
        "        txt_feat = model.txt_proj(txt_feat)\n",
        "        txt_feat = F.normalize(txt_feat, dim=-1).cpu()\n",
        "        text_embs.append(txt_feat)\n",
        "\n",
        "text_embs = torch.cat(text_embs, dim=0)\n",
        "\n",
        "image_embs_np = image_embs.numpy()\n",
        "text_embs_np = text_embs.numpy()\n",
        "\n",
        "print(\"Embedding cache ready.\")"
      ],
      "metadata": {
        "id": "FiuUDU2B4yAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Retrieval Helper Functions\n",
        "# ============================================================\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "def show_image(path, title=None, figsize=(6,6)):\n",
        "    im = Image.open(path).convert('RGB')\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.imshow(im)\n",
        "    plt.axis('off')\n",
        "    if title:\n",
        "        plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def get_caption(j):\n",
        "    return captions[j]\n",
        "\n",
        "\n",
        "def get_image_path(i):\n",
        "    return image_paths[i]\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Image → Top-K Captions\n",
        "# ------------------------------------------------------------\n",
        "def image_to_topk_captions(image_input, k=5, return_scores=False):\n",
        "\n",
        "    if isinstance(image_input, str):\n",
        "        im = Image.open(image_input).convert('RGB')\n",
        "        im_t = preprocess(im).unsqueeze(0).to(device)\n",
        "    else:\n",
        "        im = image_input\n",
        "        im_t = preprocess(im).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        img_feat = model.clip_model.encode_image(im_t)\n",
        "        img_feat = model.img_proj(img_feat)\n",
        "        img_feat = F.normalize(img_feat, dim=-1).cpu().numpy()[0]\n",
        "\n",
        "    sims = text_embs_np @ img_feat\n",
        "    topk_idx = np.argsort(-sims)[:k]\n",
        "\n",
        "    results = [(get_caption(j), float(sims[j])) for j in topk_idx]\n",
        "\n",
        "    if return_scores:\n",
        "        return results\n",
        "    return [r[0] for r in results]\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Text → Top-K Images\n",
        "# ------------------------------------------------------------\n",
        "def text_to_topk_images(text, k=5, return_scores=False):\n",
        "\n",
        "    tokenized = clip.tokenize([text], truncate=True).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        txt_feat = model.clip_model.encode_text(tokenized)\n",
        "        txt_feat = model.txt_proj(txt_feat)\n",
        "        txt_feat = F.normalize(txt_feat, dim=-1).cpu().numpy()[0]\n",
        "\n",
        "    sims = image_embs_np @ txt_feat\n",
        "    topk_idx = np.argsort(-sims)[:k]\n",
        "\n",
        "    results = [(get_image_path(i), float(sims[i])) for i in topk_idx]\n",
        "\n",
        "    if return_scores:\n",
        "        return results\n",
        "    return [r[0] for r in results]"
      ],
      "metadata": {
        "id": "DJ-PN9sr42cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Retrieval-Based VQA (Caption Matching)\n",
        "# ============================================================\n",
        "\n",
        "def vqa_via_captions(image_path, question, topk_caps=10):\n",
        "\n",
        "    # Step 1: Retrieve top-k captions for the image\n",
        "    top_caps = image_to_topk_captions(image_path,\n",
        "                                      k=topk_caps,\n",
        "                                      return_scores=False)\n",
        "\n",
        "    if not top_caps:\n",
        "        return \"No captions available.\"\n",
        "\n",
        "    # Step 2: Compare question embedding to each candidate caption\n",
        "    candidates = [question] + top_caps\n",
        "    tokenized = clip.tokenize(candidates, truncate=True).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        feats = model.clip_model.encode_text(tokenized)\n",
        "        feats = model.txt_proj(feats)\n",
        "        feats = F.normalize(feats, dim=-1).cpu().numpy()\n",
        "\n",
        "    q_feat = feats[0]\n",
        "    cap_feats = feats[1:]\n",
        "\n",
        "    sims = cap_feats @ q_feat\n",
        "    best_idx = int(np.argmax(sims))\n",
        "\n",
        "    return {\n",
        "        \"question\": question,\n",
        "        \"answer_caption\": top_caps[best_idx],\n",
        "        \"score\": float(sims[best_idx]),\n",
        "        \"top_candidate_captions\": list(zip(top_caps, sims.tolist()))\n",
        "    }"
      ],
      "metadata": {
        "id": "SKJdrNq844W5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Demo 1: Image → Text Retrieval\n",
        "# ============================================================\n",
        "\n",
        "example_index = 100\n",
        "example_image = image_paths[example_index]\n",
        "\n",
        "print(\"Image → Text Retrieval Example\")\n",
        "show_image(example_image)\n",
        "\n",
        "topcaps = image_to_topk_captions(example_image,\n",
        "                                 k=5,\n",
        "                                 return_scores=True)\n",
        "\n",
        "for cap, score in topcaps:\n",
        "    print(f\"score={score:.4f} -> {cap}\")"
      ],
      "metadata": {
        "id": "pM46gl4o45pw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Demo 2: Text → Image Retrieval\n",
        "# ============================================================\n",
        "\n",
        "query_text = \"person with red hat dancing\"\n",
        "print(\"Text Query:\", query_text)\n",
        "\n",
        "top_images = text_to_topk_images(query_text,\n",
        "                                 k=5,\n",
        "                                 return_scores=True)\n",
        "\n",
        "for i, (img_path, score) in enumerate(top_images):\n",
        "    print(f\"Rank {i+1} | score={score:.4f}\")\n",
        "    show_image(img_path)"
      ],
      "metadata": {
        "id": "83Ms1yd-49ey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Demo 3: VQA\n",
        "# ============================================================\n",
        "\n",
        "question = \"What is the child doing?\"\n",
        "\n",
        "show_image(example_image)\n",
        "\n",
        "vqa_res = vqa_via_captions(example_image,\n",
        "                           question,\n",
        "                           topk_caps=10)\n",
        "\n",
        "print(\"Question:\", question)\n",
        "print(\"Answer:\", vqa_res[\"answer_caption\"])\n",
        "print(\"Score:\", vqa_res[\"score\"])"
      ],
      "metadata": {
        "id": "me1vXzte4-8v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
