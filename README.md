# Fine-Grained Multimodal Alignment with Cross-Attention Using CLIP for Enhanced Retrieval

Official implementation of:

**â€œFine-Grained Multimodal Alignment with Cross-Attention Using CLIP for Enhanced Retrievalâ€**

---

## ğŸ” Abstract

Large-scale visionâ€“language models such as CLIP learn strong global imageâ€“text representations when trained on massive datasets. However, their performance deteriorates under low-resource conditions such as Flickr8k (8,092 images, 5 captions per image).

This work proposes a lightweight two-stage alignment framework:

1. **Stage 1:** Gentle global fine-tuning of CLIP on Flickr8k  
2. **Stage 2:** A compact cross-attention module enabling fine-grained regionâ€“token alignment between:
   - Region features extracted using Faster R-CNN  
   - Token-level text embeddings from CLIP  

The method improves retrieval performance significantly under limited data conditions while remaining computationally efficient.

---

## ğŸ§  Methodology

### Stage 1 â€” Global Fine-Tuning

We fine-tune CLIP using contrastive learning.

Global similarity:

s_global(I, T) = cos(g_I, g_T) / Ï„

where:
- g_I = image embedding  
- g_T = text embedding  
- Ï„ = temperature (0.07)

We optimize the standard NT-Xent contrastive loss.

Training Details:
- Optimizer: AdamW
- Learning rate: 1e-5
- Weight decay: 1e-4
- Batch size: 64
- Temperature: 0.07
- Epochs: 12
- Mixed Precision (AMP)
- Random seed: 42

---

### Stage 2 â€” Fine-Grained Cross-Attention Alignment

Global embeddings treat the image as a single vector.  
To enable detailed grounding, we align region-level features with token-level embeddings.

Let:
- r_k âˆˆ R^d = region feature from Faster R-CNN  
- t_m âˆˆ R^d = token embedding  

Cross-attention weights:

Î±_mk = exp(t_máµ€ W_qáµ€ W_k r_k)  
       / Î£_k' exp(t_máµ€ W_qáµ€ W_k r_k')

Region aggregation:

v_m = Î£_k Î±_mk W_v r_k

Fine-grained similarity:

s_fine(I, T) = (1/M) Î£_m t_máµ€ W_o v_m

Final similarity:

s(I, T) = s_global(I, T) + Î» s_fine(I, T)

In all experiments:

Î» = 1.0

During Stage 2:
- CLIP backbones are frozen  
- Only cross-attention and projection layers are updated  

---

## ğŸ“Š Dataset

**Flickr8k**
- 8,092 images
- 40,460 captions (5 per image)
- Standard split:
  - 6,000 training
  - 1,000 validation
  - 1,000 test

This represents a low-resource multimodal setting.

---

## ğŸ“ˆ Experimental Results

### Text-to-Image Retrieval

| Model | R@1 | R@5 | R@10 |
|-------|------|------|------|
| CLIP Zero-Shot | 50.50% | 70.23% | 83.45% |
| CLIP Global Fine-Tuned | 71.41% | 89.25% | 91.87% |
| **Proposed (Global + Fine-Grained)** | **75.59%** | **92.33%** | **93.73%** |

---

### Image-to-Text Retrieval

| Model | R@1 | R@5 | R@10 |
|-------|------|------|------|
| CLIP Zero-Shot | 56.12% | 81.61% | 89.78% |
| CLIP Global Fine-Tuned | 83.06% | 84.27% | 88.48% |
| **Proposed (Global + Fine-Grained)** | **87.21%** | **89.88%** | **92.88%** |

---

## ğŸ§ª Ablation Study

| Model Variant | Textâ†’Image R@1 |
|---------------|----------------|
| Global Fine-Tuning Only | 71.41% |
| + Cross-Attention (Î» = 1.0) | **75.59%** |

Fine-grained alignment provides consistent improvements over global-only adaptation.

---

## ğŸ–¼ Retrieval & VQA

The notebook includes:

- Image â†’ Top-K Caption Retrieval  
- Text â†’ Top-K Image Retrieval  
- Retrieval-based Visual Question Answering (VQA)

For VQA:
- The question embedding is matched with caption embeddings  
- The most similar caption is returned as the answer  
- No separate answer classifier is trained  

This keeps the system lightweight and stable.

---


## ğŸ¯ Key Contributions

- Lightweight two-stage adaptation framework  
- Explicit regionâ€“token cross-attention alignment  
- Significant Recall@1 improvement under limited data  
- Stable and computationally efficient design  

---

## âš™ï¸ Computational Efficiency

- Stage 2 updates only lightweight cross-attention layers  
- Faster R-CNN region features are precomputed and reused  
- No heavy multimodal fusion backbone  
- Stable under low-resource conditions  

---

## ğŸš€ How to Run (Colab)

1. Open the notebook in Google Colab  
2. Mount Google Drive  
3. Set:
